# IVF-Flat GPU Search - Shared Memory å„ªåŒ–æ–¹æ¡ˆ

## å•é¡Œæè¿°

ç•¶ `k=100` æ™‚ï¼ŒKernel C å’Œ Kernel D çš„ shared memory ä½¿ç”¨é‡è¶…éç¡¬é«”é™åˆ¶ï¼ˆ48KBï¼‰ï¼š

### Kernel C çš„å•é¡Œ

```
shared memory éœ€æ±‚ = dim * sizeof(float) + k * block_size * sizeof(DistIdPair)
                   = 128 * 4 + 100 * 128 * 8
                   = 512 + 102,400
                   = ~100 KB âŒ è¶…é 48KB é™åˆ¶
```

**åŸå› **ï¼š
- Query vector: `dim * sizeof(float)` (é€šå¸¸ 512 bytesï¼Œé‚„å¥½)
- Merge buffer: `k * block_size * sizeof(DistIdPair)` (k=100, block_size=128 æ™‚éœ€è¦ 100KB)

### Kernel D çš„å•é¡Œ

```
shared memory éœ€æ±‚ = nprobe * k * sizeof(DistIdPair)
                   = 64 * 100 * 8
                   = 51,200 bytes = 50 KB âŒ è¶…é 48KB é™åˆ¶
```

**åŸå› **ï¼š
- éœ€è¦å°‡æ‰€æœ‰ partial candidates è¼‰å…¥ shared memory åš merge

---

## è§£æ±ºæ–¹æ¡ˆæ¦‚è¦½

æˆ‘å€‘æä¾›ä¸‰ç¨®æ–¹æ¡ˆï¼Œå¾ç°¡å–®åˆ°è¤‡é›œï¼š

| æ–¹æ¡ˆ | è¤‡é›œåº¦ | æ•ˆèƒ½å½±éŸ¿ | é©ç”¨æƒ…å¢ƒ |
|------|--------|---------|---------|
| æ–¹æ¡ˆ 1: å…©éšæ®µ Kernel C | ä¸­ | å° | k > 50 |
| æ–¹æ¡ˆ 2: Global Memory Merge | ä½ | ä¸­ç­‰ | k > 50 |
| æ–¹æ¡ˆ 3: å‹•æ…‹ç­–ç•¥é¸æ“‡ | é«˜ | æœ€å° | ç”Ÿç”¢ç’°å¢ƒ |

---

## æ–¹æ¡ˆ 1: å…©éšæ®µ Kernel C (æ¨è–¦)

### æ ¸å¿ƒæ€æƒ³

å°‡åŸæœ¬çš„ Kernel C æ‹†æˆå…©å€‹éšæ®µï¼š

1. **Kernel C-Scan**: æ¯å€‹ thread æƒæ listï¼Œç¶­è­· thread-local top-kï¼Œç›´æ¥å¯«åˆ° global memory
2. **Kernel C-Merge**: åˆä½µåŒä¸€å€‹ (query, probe) çš„æ‰€æœ‰ thread results

### å„ªé»

- å®Œå…¨é¿å… shared memory é™åˆ¶
- ä¿æŒåŸæœ‰çš„ä¸¦è¡Œåº¦
- åªå¢åŠ ä¸€æ¬¡ global memory å¯«å…¥/è®€å–

### ç¼ºé»

- éœ€è¦é¡å¤–çš„ global memory ç©ºé–“ï¼š`Q * nprobe * block_size * k * sizeof(DistIdPair)`
- å¤šä¸€æ¬¡ kernel launch (overhead å¾ˆå°)

---

## å¯¦ä½œç´°ç¯€

### 1. æ–°å¢è¼”åŠ©è³‡æ–™çµæ§‹

åœ¨ `batch_search_gpu_pipeline_v2` ä¸­å¢åŠ ä¸­é–“ bufferï¼š

```cpp
// æ–°å¢: thread-level top-k å„²å­˜ç©ºé–“
DistIdPair *d_thread_topk;
int threads_per_block = 128;
size_t thread_topk_size = num_queries * nprobe * threads_per_block * k * sizeof(DistIdPair);

CUDA_CHECK(cudaMalloc(&d_thread_topk, thread_topk_size));
```

---

### 2. ä¿®æ”¹ Kernel C - åªåš Scanï¼Œä¸åš Merge

**å‡½æ•¸ç°½å**ï¼š

```cuda
__global__ void kernel_c_scan_lists_v2(
    const float* __restrict__ queries,        // [Q x dim]
    const int* __restrict__ selected_lists,   // [Q x nprobe]
    const float* __restrict__ vectors,        // [N_total x dim]
    const int* __restrict__ list_offsets,     // [nlist + 1]
    const size_t* __restrict__ ids,           // [N_total]
    DistIdPair* __restrict__ thread_topk,     // [Q x nprobe x block_size x k] OUTPUT
    int num_queries,
    int nprobe,
    int k,
    int dim
)
```

**ä¸»è¦æ”¹å‹•**ï¼š

```cuda
{
    // ... å‰é¢çš„é‚è¼¯ç›¸åŒ ...

    // Thread-local top-k (in registers)
    DistIdPair local_topk[MAX_K];
    int local_size = 0;
    int max_local_k = min(k, MAX_K);

    // Scan list (èˆ‡åŸç‰ˆç›¸åŒ)
    for (int idx = list_start + tid; idx < list_end; idx += block_size) {
        size_t vec_id = ids[idx];
        const float* vec_ptr = vectors + vec_id * dim;

        float sum = 0.0f;
        #pragma unroll 4
        for (int d = 0; d < dim; ++d) {
            float diff = shared_query[d] - vec_ptr[d];
            sum += diff * diff;
        }

        insert_to_local_topk(local_topk, local_size, max_local_k, sum, vec_id);
    }

    // ========== ä¿®æ”¹é»: ç›´æ¥å¯«åˆ° global memory ==========

    // è¨ˆç®—é€™å€‹ thread åœ¨ global buffer ä¸­çš„ä½ç½®
    int thread_global_idx = (q * nprobe + probe) * blockDim.x + tid;
    DistIdPair* my_output = thread_topk + thread_global_idx * k;

    // å¯«å…¥ thread-local top-k
    for (int i = 0; i < local_size; ++i) {
        my_output[i] = local_topk[i];
    }

    // å¡«å……å‰©é¤˜ä½ç½®ç‚º INFINITY
    for (int i = local_size; i < k; ++i) {
        my_output[i] = DistIdPair();  // (INFINITY, -1)
    }

    // ä¸éœ€è¦ __syncthreads() å’Œ merge_block_topk()
}
```

**Shared Memory éœ€æ±‚**ï¼š

```
åªéœ€è¦ query vector: dim * sizeof(float) (é€šå¸¸ < 1KB) âœ…
```

---

### 3. æ–°å¢ Kernel C-Merge - åˆä½µ Thread Results

**å‡½æ•¸ç°½å**ï¼š

```cuda
__global__ void kernel_c_merge_thread_topk(
    const DistIdPair* __restrict__ thread_topk,  // [Q x nprobe x block_size x k]
    DistIdPair* __restrict__ partial_topk,       // [Q x nprobe x k] OUTPUT
    int num_queries,
    int nprobe,
    int k,
    int threads_per_block
)
```

**å¯¦ä½œ (ç°¡å–®ç‰ˆæœ¬)**ï¼š

```cuda
{
    int q = blockIdx.x;
    int probe = blockIdx.y;

    if (q >= num_queries || probe >= nprobe) return;

    int tid = threadIdx.x;

    // ç¸½å…±æœ‰ threads_per_block * k å€‹å€™é¸
    int total_candidates = threads_per_block * k;
    const DistIdPair* input_base = thread_topk + (q * nprobe + probe) * total_candidates;

    // ä½¿ç”¨ thread 0 åšç°¡å–®çš„ k-pass selection
    // (å¯ä»¥å„ªåŒ–æˆå¤šå€‹ threads å”ä½œï¼Œä½† k=100 æ™‚å–® thread ä¹Ÿå¤ å¿«)
    if (tid == 0) {
        DistIdPair best_k[MAX_K];
        bool used[MAX_CANDIDATES];  // æˆ–ç”¨å…¶ä»–æ–¹å¼æ¨™è¨˜

        // åˆå§‹åŒ–
        for (int i = 0; i < total_candidates; ++i) {
            used[i] = false;
        }

        // k è¼ªé¸æ“‡
        for (int round = 0; round < k; ++round) {
            DistIdPair best = DistIdPair();
            int best_idx = -1;

            // ç·šæ€§æƒææ‰¾æœ€å°å€¼
            for (int i = 0; i < total_candidates; ++i) {
                if (!used[i] && input_base[i] < best) {
                    best = input_base[i];
                    best_idx = i;
                }
            }

            if (best_idx >= 0 && best.id >= 0) {
                best_k[round] = best;
                used[best_idx] = true;
            } else {
                best_k[round] = DistIdPair();
            }
        }

        // å¯«å‡ºçµæœ
        DistIdPair* output = partial_topk + (q * nprobe + probe) * k;
        for (int i = 0; i < k; ++i) {
            output[i] = best_k[i];
        }
    }
}
```

**Grid/Block é…ç½®**ï¼š

```cpp
dim3 grid_size(num_queries, nprobe);
int block_size = 32;  // åªéœ€è¦å°‘é‡ threads (ç”šè‡³åªç”¨ 1 å€‹)

kernel_c_merge_thread_topk<<<grid_size, block_size>>>(
    d_thread_topk, d_partial_topk,
    num_queries, nprobe, k, threads_per_block_in_scan
);
```

**è¤‡é›œåº¦åˆ†æ**ï¼š

- æ¯å€‹ block è™•ç† `threads_per_block * k` å€‹å€™é¸ (ä¾‹å¦‚ 128 * 100 = 12,800)
- k è¼ªé¸æ“‡ï¼Œæ¯è¼ª O(threads_per_block * k)
- ç¸½è¤‡é›œåº¦ï¼šO(kÂ² * threads_per_block) â‰ˆ O(1.28M) operations
- å°æ–¼ GPU ä¾†èªªé€™ä¸ç®—å¤šï¼Œä¸”æ˜¯ embarrassingly parallel

---

### 4. ä¿®æ”¹ Kernel D - ä½¿ç”¨ Global Memory æˆ– Optimized Selection

#### é¸é … A: ç°¡å–®ç‰ˆæœ¬ - å–® Thread Selection

```cuda
__global__ void kernel_d_merge_final_topk_v2(
    const DistIdPair* __restrict__ partial_topk,
    float* __restrict__ out_distances,
    int* __restrict__ out_indices,
    int num_queries,
    int nprobe,
    int k
) {
    int q = blockIdx.x;
    if (q >= num_queries) return;

    int tid = threadIdx.x;

    if (tid == 0) {
        int total_candidates = nprobe * k;
        const DistIdPair* input = partial_topk + q * total_candidates;

        // ç°¡å–®çš„ k-pass selection (ä¸ä½¿ç”¨ shared memory)
        bool used[MAX_CANDIDATES];
        for (int i = 0; i < total_candidates; ++i) {
            used[i] = false;
        }

        for (int round = 0; round < k; ++round) {
            DistIdPair best = DistIdPair();
            int best_idx = -1;

            for (int i = 0; i < total_candidates; ++i) {
                if (!used[i] && input[i] < best) {
                    best = input[i];
                    best_idx = i;
                }
            }

            if (best_idx >= 0 && best.id >= 0) {
                out_distances[q * k + round] = best.dist;
                out_indices[q * k + round] = best.id;
                used[best_idx] = true;
            } else {
                out_distances[q * k + round] = INFINITY;
                out_indices[q * k + round] = -1;
            }
        }
    }
}
```

**å•é¡Œ**ï¼š`used` é™£åˆ—å¤ªå¤§ (nprobe * k å¯èƒ½åˆ° 6400)

#### é¸é … B: å„ªåŒ–ç‰ˆæœ¬ - Heap-based Selection

```cuda
__global__ void kernel_d_merge_final_topk_heap(
    const DistIdPair* __restrict__ partial_topk,
    float* __restrict__ out_distances,
    int* __restrict__ out_indices,
    int num_queries,
    int nprobe,
    int k
) {
    int q = blockIdx.x;
    if (q >= num_queries) return;

    int tid = threadIdx.x;

    if (tid == 0) {
        int total_candidates = nprobe * k;
        const DistIdPair* input = partial_topk + q * total_candidates;

        // ä½¿ç”¨ min-heap (size = k) åš streaming selection
        DistIdPair heap[MAX_K];
        int heap_size = 0;

        // éæ­·æ‰€æœ‰ candidates
        for (int i = 0; i < total_candidates; ++i) {
            DistIdPair cand = input[i];
            if (cand.id < 0) continue;  // è·³éç„¡æ•ˆé …

            if (heap_size < k) {
                // Heap é‚„æ²’æ»¿ï¼Œç›´æ¥æ’å…¥
                heap[heap_size++] = cand;
                if (heap_size == k) {
                    // å»ºç«‹ min-heap
                    for (int j = k/2 - 1; j >= 0; --j) {
                        heapify_down(heap, k, j);
                    }
                }
            } else if (cand < heap[0]) {
                // æ–°å…ƒç´ æ¯” heap é ‚å°ï¼Œæ›¿æ›ä¸¦é‡æ–° heapify
                heap[0] = cand;
                heapify_down(heap, k, 0);
            }
        }

        // æ’åº heap ä¸¦è¼¸å‡º
        for (int i = heap_size - 1; i > 0; --i) {
            swap(heap[0], heap[i]);
            heapify_down(heap, i, 0);
        }

        for (int i = 0; i < heap_size; ++i) {
            out_distances[q * k + i] = heap[i].dist;
            out_indices[q * k + i] = heap[i].id;
        }
        for (int i = heap_size; i < k; ++i) {
            out_distances[q * k + i] = INFINITY;
            out_indices[q * k + i] = -1;
        }
    }
}

// è¼”åŠ©å‡½æ•¸
__device__ void heapify_down(DistIdPair* heap, int size, int i) {
    int largest = i;
    int left = 2 * i + 1;
    int right = 2 * i + 2;

    if (left < size && heap[left].dist < heap[largest].dist) {
        largest = left;
    }
    if (right < size && heap[right].dist < heap[largest].dist) {
        largest = right;
    }

    if (largest != i) {
        DistIdPair tmp = heap[i];
        heap[i] = heap[largest];
        heap[largest] = tmp;
        heapify_down(heap, size, largest);
    }
}

__device__ void swap(DistIdPair& a, DistIdPair& b) {
    DistIdPair tmp = a;
    a = b;
    b = tmp;
}
```

**è¤‡é›œåº¦**ï¼šO(total_candidates * log k) = O(nprobe * k * log k)

---

## æ–¹æ¡ˆ 2: Global Memory Merge (æœ€ç°¡å–®)

å¦‚æœä¸æƒ³æ”¹å¤ªå¤šï¼Œå¯ä»¥åªä¿®æ”¹ Kernel Dï¼š

### å¯¦ä½œæ­¥é©Ÿ

1. ä¿æŒ Kernel C ä¸è®Š
2. ä¿®æ”¹ Kernel D ä½¿ç”¨é¸é … B (Heap-based)
3. èª¿æ•´ Kernel C çš„ block_size ä½¿å…¶ shared memory ä¸è¶…éé™åˆ¶

### å‹•æ…‹èª¿æ•´ Block Size

```cpp
// åœ¨ batch_search_gpu_pipeline_v2 ä¸­
{
    dim3 grid_size(num_queries, nprobe);

    // è¨ˆç®—æœ€å¤§å¯ç”¨çš„ block_size
    const size_t MAX_SMEM = 48 * 1024;
    size_t query_smem = dim * sizeof(float);
    size_t available = MAX_SMEM - query_smem;

    // merge buffer éœ€è¦ k * block_size * sizeof(DistIdPair)
    int max_block_size = available / (k * sizeof(DistIdPair));
    max_block_size = (max_block_size / 32) * 32;  // Round down to multiple of 32

    int block_size;
    if (max_block_size >= 32) {
        block_size = min(128, max_block_size);
    } else {
        // k å¤ªå¤§ï¼Œç„¡æ³•åœ¨ shared memory åš merge
        // é™ç´šç‚ºåªæƒæï¼Œä¸åš block-level merge
        block_size = 128;
        // ... éœ€è¦ä¿®æ”¹ kernel æˆ–ä½¿ç”¨æ–¹æ¡ˆ 1
    }

    size_t smem_size = query_smem + k * block_size * sizeof(DistIdPair);

    kernel_c_scan_lists<<<grid_size, block_size, smem_size>>>(
        d_queries, d_selected_lists, d_base_data, d_list_offsets,
        d_list_data, d_partial_topk,
        num_queries, nprobe, k, dim
    );
}
```

---

## æ–¹æ¡ˆ 3: å‹•æ…‹ç­–ç•¥é¸æ“‡ (ç”Ÿç”¢ç’°å¢ƒæ¨è–¦)

æ ¹æ“š k å€¼è‡ªå‹•é¸æ“‡æœ€ä½³ç­–ç•¥ï¼š

```cpp
void batch_search_gpu_pipeline_adaptive(
    const float* queries,
    const float* d_centroids,
    const float* d_base_data,
    const size_t* d_list_data,
    const int* d_list_offsets,
    float* result_distances,
    size_t* result_indices,
    size_t num_queries,
    size_t num_centroids,
    size_t nprobe,
    size_t k,
    size_t dim
) {
    // æ±ºå®šä½¿ç”¨å“ªç¨®ç­–ç•¥
    const size_t MAX_SMEM = 48 * 1024;
    size_t kernel_c_smem = dim * sizeof(float) + k * 128 * sizeof(DistIdPair);
    size_t kernel_d_smem = nprobe * k * sizeof(DistIdPair);

    bool use_two_stage = (kernel_c_smem > MAX_SMEM) || (kernel_d_smem > MAX_SMEM);

    if (use_two_stage) {
        // ä½¿ç”¨æ–¹æ¡ˆ 1: å…©éšæ®µ merge
        batch_search_gpu_pipeline_two_stage(
            queries, d_centroids, d_base_data, d_list_data, d_list_offsets,
            result_distances, result_indices,
            num_queries, num_centroids, nprobe, k, dim
        );
    } else {
        // ä½¿ç”¨åŸç‰ˆ (ä¸€éšæ®µ shared memory merge)
        batch_search_gpu_pipeline_v2(
            queries, d_centroids, d_base_data, d_list_data, d_list_offsets,
            result_distances, result_indices,
            num_queries, num_centroids, nprobe, k, dim
        );
    }
}
```

---

## æ•ˆèƒ½é æœŸ

### æ–¹æ¡ˆ 1 (å…©éšæ®µ)

| éšæ®µ | é¡å¤–é–‹éŠ· |
|------|---------|
| Kernel C-Scan | ç„¡ (èˆ‡åŸç‰ˆç›¸åŒ) |
| Global memory write | ~1-2ms (å°æ–¼å¤§ batch) |
| Kernel C-Merge launch | ~0.01ms |
| Kernel C-Merge compute | ~0.5-1ms |
| **ç¸½é¡å¤–é–‹éŠ·** | **~2-4ms** |

å°æ–¼ k=100 çš„æƒ…æ³ï¼Œé€™å€‹é–‹éŠ·æ˜¯å¯æ¥å—çš„ã€‚

### æ–¹æ¡ˆ 2 (Global Memory)

| éšæ®µ | é¡å¤–é–‹éŠ· |
|------|---------|
| Kernel C (reduced block_size) | +10-20% (å› ç‚ºä¸¦è¡Œåº¦ä¸‹é™) |
| Kernel D (heap-based) | -10% (åè€Œæ›´å¿«ï¼Œå› ç‚ºé¿å… shared memory çˆ­ç”¨) |
| **ç¸½é¡å¤–é–‹éŠ·** | **~5-10%** |

---

## å¯¦ä½œå„ªå…ˆé †åº

### Phase 1: å¿«é€Ÿä¿®å¾© (1-2 å°æ™‚)
- âœ… å¯¦ä½œ Kernel D heap-based ç‰ˆæœ¬
- âœ… æ·»åŠ å‹•æ…‹ block_size èª¿æ•´
- âœ… æ¸¬è©¦ k=100 æ˜¯å¦èƒ½è·‘

### Phase 2: å®Œæ•´æ–¹æ¡ˆ (3-4 å°æ™‚)
- âœ… å¯¦ä½œå…©éšæ®µ Kernel C
- âœ… å¯¦ä½œ Kernel C-Merge
- âœ… æ•´åˆåˆ° batch_search_gpu_pipeline_v2
- âœ… å®Œæ•´æ¸¬è©¦èˆ‡ benchmark

### Phase 3: å„ªåŒ– (é¸åš)
- ğŸ”„ Kernel C-Merge ä½¿ç”¨å¤š threads å”ä½œ
- ğŸ”„ ä½¿ç”¨ warp-level primitives å„ªåŒ–
- ğŸ”„ å¯¦ä½œ adaptive strategy selection

---

## åƒè€ƒè³‡æ–™

- **CUDA Shared Memory Limit**: 48KB (Compute Capability 3.x-8.x)
- **Max Shared Memory (dynamic config)**: 96KB (Compute Capability 7.0+, éœ€è¦ `cudaFuncSetAttribute`)
- **Alternative**: ä½¿ç”¨ L1 cache (è‡ªå‹•ç®¡ç†)

---

## ä¸‹ä¸€æ­¥

å»ºè­°å¾ **Phase 1** é–‹å§‹å¯¦ä½œï¼Œé€™æ¨£å¯ä»¥å¿«é€Ÿé©—è­‰è§£æ±ºæ–¹æ¡ˆæ˜¯å¦å¯è¡Œã€‚
